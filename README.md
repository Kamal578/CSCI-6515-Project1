# Spring 2026 Natural Language Processing (CSCI-6515)

# Project 1: Azerbaijani Wikipedia Corpus and NLP Pipeline

## Authors
- Kamal Ahmadov (kahmadov24700@ada.edu.az; kamal.ahmadov@gwu.edu)
- Rufat Guliyev (rguliyev24988@ada.edu.az; rufat.guliyev@gwu.edu)

End-to-end mini-pipeline for collecting, cleaning, and exploring an Azerbaijani Wikipedia corpus. It covers tokenization, frequency stats (Zipf), Heaps' law fit, BPE, sentence segmentation, and spell checking (uniform + weighted edit distance), with reproducible scripts and report-ready outputs.

## Repo Layout
- `src/pull_wikipedia.py` — collect and clean Azerbaijani Wikipedia pages via the MediaWiki API.
- `src/tokenize.py` — Unicode-aware tokenization plus Wikipedia-specific cleanup helpers.
- `src/task1_stats.py` — token/type counts, frequency table, optional Zipf plot.
- `src/heaps.py` — Heaps' law (V = k * N^beta) estimation and log-log plot.
- `src/task3_bpe.py` and `src/bpe.py` — train BPE merges and encode the corpus; export merges and token frequencies.
- `src/sentence_segment.py` — rule-based sentence segmentation CLI (handles abbreviations, decimals, initials, quotes, lowercase continuations).
- `src/spellcheck.py` — Levenshtein/weighted spell checker CLI.
- `src/make_spell_test.py`, `src/confusion.py`, `src/eval_spellcheck.py`, `src/plot_confusion.py` — synthetic spell benchmark, confusion weights, evaluation, and heatmap.
- `src/build_vocab.py` — filtered vocabulary builder.
- `data/` — input data; expects `data/raw/corpus.csv` created by the collector.
- `outputs/` — auto-created results (`stats/`, `plots/`, `bpe/`, `spellcheck/`, etc.).
- `notebooks/Main.ipynb` — scratchpad/EDA; mirrors the script workflow.
- `DATASHEET.md` — dataset notes (motivation, licensing, caveats).
- `scripts/run_all.sh` — one-shot pipeline (stats, Heaps, BPE, vocab, spell eval, segmentation, summary).

## Setup
```
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\\Scripts\\activate
pip install -r requirements.txt
```
Dependencies: requests, tqdm, mwparserfromhell, regex, numpy, pandas, matplotlib.

Use Python 3.10+ for best compatibility.

After setup, ensure `data/raw/corpus.csv` exists (see collection step) before running analytics.

## 1) Collect a Corpus
Fetch and clean Azerbaijani Wikipedia pages into a CSV with one row per document.

Examples:
```
# Random sample of 800 articles
python -m src.pull_wikipedia --random 800 --out data/raw/corpus.csv

# Up to 500 articles from a category
python -m src.pull_wikipedia --category "Azərbaycan" --limit 500 --out data/raw/corpus.csv
```
Key flags:
- `--min_chars` (default 400) drops very short pages after cleaning.
- `--sleep` (default 0.1s) is a politeness delay between API batches.

Output schema: `doc_id, page_id, title, revision_id, timestamp, source, url, text` (UTF-8 CSV).

## 2) Token Stats and Zipf (Task 1)
Compute token frequencies and basic corpus stats using the tokenizer in `src/tokenize.py`.
```
python -m src.task1_stats --corpus_path data/raw/corpus.csv \
    --out_dir outputs/stats --plots_dir outputs/plots --top_n 2000
```
Outputs:
- `outputs/stats/summary.json` — documents, token/type counts, top 20 tokens, lowercase flag.
- `outputs/stats/token_freq.csv` — full frequency table.
- `outputs/plots/zipf.png` — rank-frequency plot (if `--make_zipf_plot` is true).

## 3) Heaps' Law Fit (Task 2)
Estimate Heaps' law parameters k and beta from streamed tokens.
```
python -m src.heaps --corpus_path data/raw/corpus.csv \
    --out_stats outputs/stats/heaps_params.json \
    --out_plot outputs/plots/heaps.png --step 1000
```
Outputs: JSON with k, beta, corpus size, and `heaps.png` log-log fit plot.

## 4) Byte-Pair Encoding (Task 3)
Train a simple BPE model on word tokens and export merges plus encoded token stats.
```
python -m src.task3_bpe --corpus_path data/raw/corpus.csv \
    --out_dir outputs/bpe --num_merges 5000 --min_word_freq 2 --sample_words 30
```
Outputs:
- `merges.txt` — merge rules in order.
- `bpe_token_freq.csv` — BPE token counts.
- `bpe_summary.json` — run metadata plus example word -> BPE segmentations.

## Sentence Segmentation
```
python -m src.sentence_segment --corpus_path data/raw/corpus.csv --limit 500 --out outputs/sentences.txt
```
Handles abbreviations, decimals, initials, quotes, and lowercase continuations after periods (e.g., “kv. verst” remains unsplit). Regression tests cover key edge cases.

## Tokenization Notes
- Uses `regex` with Unicode properties; keeps Azerbaijani letters, apostrophes, hyphens, and numbers (including decimals).
- Light Wikipedia cleanup (`strip_wiki_garbage`) removes category/navigation noise and normalizes punctuation.
- Toggle lowercasing via `--lowercase` where available in scripts.

## Notebook
`notebooks/Main.ipynb` mirrors the scripts with richer explanations and plots. Run after installing the requirements (Jupyter is not auto-installed—use `pip install notebook` if needed).

## Data and Licensing
- Source text: Azerbaijani Wikipedia; content is CC BY-SA. Respect attribution and ShareAlike when redistributing derived corpora.
- See `DATASHEET.md` for open issues (coverage, biases, timestamps, intended use).

## Troubleshooting
- Missing corpus error -> run the collector to create `data/raw/corpus.csv`.
- Slow downloads -> lower `--limit` / `--random` or increase `--sleep` for API politeness.
- Matplotlib backend issues in headless environments -> set `MPLBACKEND=Agg` before running plotting scripts.

## Quickstart (TL;DR)
```
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
python -m src.pull_wikipedia --random 500 --out data/raw/corpus.csv
python -m src.task1_stats
python -m src.heaps
python -m src.task3_bpe
```

## One-shot full pipeline
Runs stats, Heaps, BPE, vocab build, synthetic spell benchmark, weighted spell eval, sentence segmentation, rare-word spell suggestions, and writes a textual summary for your report.
```
bash scripts/run_all.sh
```
Key outputs:
- Plots: `outputs/plots/zipf.png`, `outputs/plots/heaps.png`
- Stats: `outputs/stats/summary.json`, `outputs/stats/heaps_params.json`, `outputs/stats/vocab_summary.json`
- BPE: `outputs/bpe/merges.txt`, `outputs/bpe/bpe_summary.json`
- Vocab: `data/processed/vocab.txt`
- Spell: `outputs/spellcheck/spell_eval.json`, `outputs/spellcheck/sample_predictions.csv`, `outputs/spellcheck/confusion.json`, `outputs/spellcheck/confusion_heatmap.png`, `outputs/spellcheck/suggestions.txt`
- Sentences: `outputs/sentences.txt`
- Report-ready summary: `outputs/run_summary.txt`

## Spellcheck example
Provide a wordlist (one word per line) and write suggestions to `outputs/spellcheck`:
```
printf "kvverst\nazrbaycan\n" > /tmp/typos.txt
python -m src.spellcheck --corpus_path data/raw/corpus.csv \
    --wordlist /tmp/typos.txt \
    --out outputs/spellcheck/typos_suggestions.txt
```

## Build a filtered vocabulary
Create a vocab with frequency/length filtering (useful for spellcheck and other tasks):
```
python -m src.build_vocab \
  --corpus_path data/raw/corpus.csv \
  --min_freq 3 \
  --min_len 3 \
  --vocab_path data/processed/vocab.txt \
  --summary_path outputs/stats/vocab_summary.json
```

## Spellcheck evaluation (weighted edit distance)
Create synthetic errors, learn confusion weights, evaluate, and plot:
```
python -m src.make_spell_test --samples 1000
python -m src.confusion --pairs_csv data/processed/spell_test.csv --out_confusion outputs/spellcheck/confusion.json
python -m src.eval_spellcheck --test_csv data/processed/spell_test.csv --confusion outputs/spellcheck/confusion.json
python -m src.plot_confusion --confusion outputs/spellcheck/confusion.json --out outputs/spellcheck/confusion_heatmap.png
```
Metrics: see `outputs/spellcheck/spell_eval.json`; heatmap at `outputs/spellcheck/confusion_heatmap.png`.
