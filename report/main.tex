\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{array}
\usepackage{enumitem}
\setlist{nosep}

% ------------------------------------------------------------------
% Quick-edit macros (fill with actual values after running pipeline)
% ------------------------------------------------------------------
\newcommand{\numdocs}{623}
\newcommand{\numtokens}{238{,}286}
\newcommand{\numtypes}{48{,}151}
\newcommand{\heapsk}{4.57}
\newcommand{\heapsbeta}{0.750}
\newcommand{\bpetypes}{5{,}239}
\newcommand{\bpemerges}{5{,}000} % default in scripts
\newcommand{\spellaccone}{0.637}
\newcommand{\spellaccfive}{0.801}

\title{Azerbaijani Wikipedia Corpus: Tokenization, Heaps' Law, BPE, Sentence Segmentation, and Spell Checking}

\author{
    Team: \\
    \\
    \textbf{Kamal Ahmadov} \\
    kahmadov24700@ada.edu.az; kamal.ahmadov@gwu.edu \\
    \and
    \textbf{Rufat Guliyev} \\
    rguliyev24988@ada.edu.az; rufat.guliyev@gwu.edu
}
\date{February 5, 2026}

\begin{document}
\maketitle

\begin{abstract}
This report presents the creation of a large-scale Azerbaijani Wikipedia corpus and an NLP pipeline that includes tokenization, frequency analysis (Zipf's Law), Heaps' law estimation, Byte Pair Encoding (BPE), rule-based sentence segmentation, and a spell-checking system based on Levenshtein distance and weighted edit distance. All processing steps were automated, and the results were saved in structured directories. The report summarizes the methods, experimental setup, results, and key metrics generated during this study.
\end{abstract}

\section{Motivation and Dataset}
\textbf{Goal:} The primary objective is to build a comprehensive Azerbaijani corpus from Wikipedia, extract lexical statistics, and create preprocessing modules for common NLP tasks such as tokenization, segmentation, and spelling correction.

\textbf{Source:} The corpus was collected using the MediaWiki API, with cleaning performed through custom scripts. The data was saved as a CSV file at \texttt{data/raw/corpus.csv} containing document IDs, titles, revision info, timestamps, URLs, and text.

\textbf{Corpus Snapshot:}
\begin{itemize}
    \item \textbf{Documents:} \numdocs
    \item \textbf{Tokens:} \numtokens
    \item \textbf{Types (unique tokens):} \numtypes
\end{itemize}

\textbf{Licensing:} The corpus and any derived datasets adhere to the \href{https://creativecommons.org/licenses/by-sa/3.0/}{CC BY-SA} license. All derived artifacts must retain proper attribution.

\section{Methods}
\subsection{Tokenization}
We employed a Unicode-aware tokenizer that retains Azerbaijani characters, including internal apostrophes and hyphens, as well as decimal numbers. Non-alphabetic characters are removed, and case normalization (lowercasing) is optional. Additionally, Wikipedia-specific preprocessing removes category/navigation lines and normalizes punctuation (\texttt{src/tokenize.py}).

\subsection{Frequency Analysis and Zipf's Law}
We performed token and type frequency analysis, generating a full frequency table and identifying the top 20 tokens, stored in \texttt{outputs/stats/token\_freq.csv} and summarized in \texttt{summary.json}.
\begin{itemize}
    \item Zipf’s Law was visualized with a rank-vs-frequency plot, available in \texttt{outputs/plots/zipf.png}. The plot confirms a typical heavy-tail distribution, which is expected in natural language datasets.
\end{itemize}

\subsection{Heaps' Law}
Heaps' Law is used to model vocabulary growth as the corpus size increases. We computed the relationship between the number of tokens \( N \) and the number of unique words \( V \), fitting the model \( V = k N^{\beta} \) via linear regression in log space (\texttt{src/heaps.py}).
\begin{itemize}
    \item Heaps' parameters: \( k = \heapsk \), \( \beta = \heapsbeta \), indicating moderately fast vocabulary growth.
    \item The plot of Heaps' law is shown in \texttt{outputs/plots/heaps.png}.
    \item The chosen \( \beta \) value reflects the broad lexical breadth of the corpus, typical of encyclopedic text.
\end{itemize}

\subsection{Byte-Pair Encoding (BPE)}
We applied BPE to segment words into subword units, which improves handling of rare and out-of-vocabulary words.
\begin{itemize}
    \item \textbf{Parameters:} 5000 merges, minimum frequency threshold of 2 (\texttt{src/task3\_bpe.py}).
    \item Outputs include: \texttt{outputs/bpe/merges.txt}, \texttt{outputs/bpe/bpe\_token\_freq.csv}, and a BPE summary in JSON format.
    \item Results: \bpetypes\ subword types and 430,034 BPE tokens. This reduces the vocabulary size and allows the model to better handle rare words.
\end{itemize}

\subsection{Sentence Segmentation}
We implemented a rule-based sentence segmenter that accounts for abbreviations, decimal numbers, initials, quote boundaries, and lowercase continuations after periods. The segmenter was tested on the first 500 documents, extracting 11,479 sentences.
\begin{itemize}
    \item Key edge cases: abbreviations (e.g., "Dr."), decimals (e.g., "3.14"), and initials (e.g., "S.Rustamov").
    \item Handling punctuation marks, especially when they appear inside quotes or are followed by a lowercase letter, posed a challenge. For instance, cases like “kv. verst” (continuation after a period) were deliberately kept unsplit.
    \item Output: \texttt{outputs/sentences.txt}.
\end{itemize}

\subsection{Spell Checking}
The spell checker uses a Levenshtein-based distance measure, enhanced with a weighted edit distance to account for common character substitutions. We evaluated the system with a synthetic test set of 1,000 misspelled words.
\begin{itemize}
    \item Vocab generated from the corpus and filtered by frequency and length; the edit distance cutoff is set to 2.
    \item Evaluation results: Accuracy@1 = \spellaccone, Accuracy@5 = \spellaccfive.
    \item The confusion matrix and weights are visualized in \texttt{outputs/spellcheck/confusion\_heatmap.png}. The heatmap helps identify the most common letter substitutions that the spellchecker struggles with.
\end{itemize}

\section{Experiments and Results}
\subsection{Collection and Cleaning}
The corpus was gathered using a random/category fetch from MediaWiki, with templates and non-relevant tags stripped using \texttt{mwparserfromhell}. Category/file links were removed, and English-heavy lines were filtered using \texttt{langid}. Documents shorter than 400 characters were discarded.
\begin{itemize}
    \item Final corpus size: \numdocs\ documents, \numtokens\ tokens, \numtypes\ types.
    \item English-heavy lines were filtered to ensure the corpus contained mostly Azerbaijani text.
\end{itemize}

\subsection{Corpus Statistics}
The token and type statistics were recorded in \texttt{outputs/run\_summary.txt}:
\begin{itemize}
    \item \textbf{Tokens:} \numtokens
    \item \textbf{Types:} \numtypes
    \item Top tokens: Refer to \texttt{outputs/stats/summary.json} for the top-20 list.
\end{itemize}

\subsection{Heaps' Law}
The vocabulary growth parameter estimates are \( k = \heapsk \), \( \beta = \heapsbeta \), reflecting a vocabulary that grows faster than the classic value of 0.5. This is consistent with the corpus's encyclopedic nature. The Heaps' Law plot is shown in Figure~\ref{fig:heaps}.

\subsection{Zipf's Law}
The Zipf plot in Figure~\ref{fig:zipf} shows a typical rank-vs-frequency distribution, with a linear region confirming Zipf's Law behavior. The dataset’s vocabulary distribution follows the expected heavy-tail structure found in many natural language corpora.

\subsection{Byte-Pair Encoding (BPE)}
The BPE model performed 5,000 merges, generating 5,239 subword types. The total number of BPE tokens emitted was 430,034. This reduces the vocabulary size and facilitates more efficient handling of unseen words. Example segmentations are available in \texttt{outputs/bpe/bpe\_summary.json}.

\subsection{Sentence Segmentation}

Sentence segmentation was performed using a rule-based approach tailored to the characteristics of Azerbaijani Wikipedia text. From the first 500 documents, the segmenter extracted 11{,}479 sentences, producing well-formed sentence boundaries in the majority of cases.

A key challenge in this setting is that punctuation alone is an unreliable indicator of sentence boundaries. Azerbaijani Wikipedia articles frequently contain decimal numbers, abbreviations, personal initials, and quoted material, all of which can lead to erroneous splits under naive punctuation-based rules.

To address these issues, we enhanced the baseline segmenter with several language-aware heuristics. First, periods and commas are not treated as sentence boundaries when they are surrounded by non-space characters on both sides. This prevents incorrect segmentation in numerical expressions (e.g., \textit{3.14}) and compact abbreviations. Second, periods following a single uppercase letter are not interpreted as sentence boundaries, which improves handling of initial-based names such as \textit{K. Ahmadov}. Third, we introduce quotation-aware rules for guillemets and quotation marks. When a quoted segment ends with sentence-final punctuation, the segmenter inspects the following context: if the closing quote is followed by a space and a lowercase letter, no sentence boundary is introduced; if followed by a space and an uppercase letter, a sentence boundary is detected.

These enhancements substantially reduce false positives while preserving true sentence boundaries, particularly in texts containing numerical expressions, personal names, and embedded citations. Qualitative inspection shows that the enhanced segmenter handles lowercase continuations after periods (e.g., \textit{kv. verst}) correctly, with minimal remaining errors.



\subsection{Spell Checking}
The Levenshtein-based spell checker, enhanced with weighted edit distance, achieved an accuracy of Accuracy@1 = \spellaccone\ and Accuracy@5 = \spellaccfive. A confusion matrix of top substitutions is visualized in Figure~\ref{fig:confusion}, highlighting common character substitutions.

\section{Reproducibility}
The full pipeline can be run with the following command:
\begin{verbatim}
bash scripts/run_all.sh
\end{verbatim}
Key outputs:
\begin{itemize}
    \item Plots: \texttt{outputs/plots/zipf.png}, \texttt{outputs/plots/heaps.png}
    \item Stats: \texttt{outputs/stats/summary.json}, \texttt{outputs/stats/heaps\_params.json}
    \item BPE: \texttt{outputs/bpe/merges.txt}, \texttt{outputs/bpe/bpe\_summary.json}
    \item Vocab: \texttt{data/processed/vocab.txt}
    \item Spellcheck eval: \texttt{outputs/spellcheck/spell\_eval.json}, \texttt{sample\_predictions.csv}, \texttt{confusion\_heatmap.png}
    \item Run summary: \texttt{outputs/run\_summary.txt}
\end{itemize}

\section{Discussion and Future Work}
\begin{itemize}
    \item Enhance data cleaning (address punctuation and diacritics).
    \item Implement more robust language-ID filtering to exclude non-Azerbaijani content.
    \item Train a neural segmenter or language model for improved performance.
    \item Expand spell checker with context-aware methods.
    \item Replace synthetic evaluation with human-annotated datasets for better performance.
\end{itemize}

\section{Figures}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{zipf.png}
    \caption{Zipf plot (rank vs.\ frequency, log--log).}
    \label{fig:zipf}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{heaps.png}
    \caption{Heaps' law fit with observed $V(N)$ and model $kN^{\beta}$.}
    \label{fig:heaps}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{confusion_heatmap.png}
    \caption{Top substitution confusions (weighted spell checker).}
    \label{fig:confusion}
\end{figure}

\end{document}
